
import { GoogleGenAI, Type } from "@google/genai";
import { Message, StudentProfile, Document, SubjectGrades } from "./types";

const getAIClient = () => {
  const apiKey = process.env.GEMINI_API_KEY;
  if (!apiKey || apiKey === "") {
    throw new Error("API_KEY_MISSING");
  }
  return new GoogleGenAI({ apiKey });
};

// H√†m ph·ª• ƒë·ªÉ t√≠nh TBM gi√∫p AI c√≥ d·ªØ li·ªáu ch√≠nh x√°c nh·∫•t
const calculateAvg = (grades: SubjectGrades) => {
  let totalPoints = 0;
  let totalWeight = 0;
  grades.frequent.filter(g => g !== null).forEach(g => { totalPoints += Number(g); totalWeight += 1; });
  if (grades.midterm !== null) { totalPoints += Number(grades.midterm) * 2; totalWeight += 2; }
  if (grades.final !== null) { totalPoints += Number(grades.final) * 3; totalWeight += 3; }
  return totalWeight === 0 ? 0 : parseFloat((totalPoints / totalWeight).toFixed(2));
};

export const generateProfileAnalysis = async (profile: StudentProfile) => {
  try {
    const ai = getAIClient();
    
    const subjectsSummary = profile.subjects
      .filter(s => s.isActive)
      .map(s => ({
        name: s.name,
        avg: calculateAvg(s.grades),
        frequent: s.grades.frequent,
        midterm: s.grades.midterm,
        final: s.grades.final
      }));

    const prompt = `
      PH√ÇN T√çCH H·ªåC B·∫† H·ªåC SINH (Th·ªùi gian: ${new Date().toLocaleString()})
      H·ªçc sinh: ${profile.name} - L·ªõp: ${profile.grade}
      M·ª•c ti√™u: ${profile.focusSubject}
      
      D·ªØ li·ªáu ƒëi·ªÉm s·ªë hi·ªán t·∫°i (ƒê√£ t√≠nh to√°n TBM):
      ${JSON.stringify(subjectsSummary)}
      
      L·ªói sai g·∫ßn ƒë√¢y: ${JSON.stringify(profile.recentErrors)}

      Y√äU C·∫¶U QUAN TR·ªåNG:
      1. ƒê∆∞a ra nh·∫≠n x√©t C√Å NH√ÇN H√ìA, S√ÅNG T·∫†O v√† KH√îNG TR√ôNG L·∫∂P.
      2. Status: M·ªôt c√¢u c·ª±c ng·∫Øn v·ªÅ phong ƒë·ªô (VD: "B·ª©t ph√° ngo·∫°n m·ª•c", "C·∫£nh b√°o sa s√∫t", "·ªîn ƒë·ªãnh").
      3. Overview: ƒê√°nh gi√° d·ª±a tr√™n TBM c√°c m√¥n so v·ªõi m·ª•c ti√™u kh·ªëi thi. H√£y d√πng vƒÉn phong kh√≠ch l·ªá nh∆∞ng th·∫≥ng th·∫Øn.
      4. Gaps: Ch·ªâ ra m√¥n n√†o c√≥ ƒëi·ªÉm th√†nh ph·∫ßn th·∫•p b·∫•t th∆∞·ªùng ho·∫∑c c·∫ßn ch√∫ √Ω.
      5. Strategy: 3 h√†nh ƒë·ªông c·ª• th·ªÉ, th·ª±c t·∫ø ƒë·ªÉ c·∫£i thi·ªán TBM. M·ªói l·∫ßn l√†m m·ªõi h√£y c·ªë g·∫Øng ƒë∆∞a ra c√°c g·ª£i √Ω ƒëa d·∫°ng h∆°n.

      Tr·∫£ v·ªÅ ƒë·ªãnh d·∫°ng JSON: { "status": string, "overview": string, "gaps": string, "strategy": string[] }
    `;

    const response = await ai.models.generateContent({
      model: "gemini-3-flash-preview",
      contents: prompt,
      config: {
        temperature: 1,
        responseMimeType: "application/json",
        responseSchema: {
          type: Type.OBJECT,
          properties: {
            status: { type: Type.STRING },
            overview: { type: Type.STRING },
            gaps: { type: Type.STRING },
            strategy: { type: Type.ARRAY, items: { type: Type.STRING } },
          },
          required: ["status", "overview", "gaps", "strategy"]
        }
      }
    });
    const text = response.text;
    if (!text) throw new Error("Empty response from AI");
    return JSON.parse(text);
  } catch (error) {
    console.error("AI Analysis Error:", error);
    // Tr·∫£ v·ªÅ null ƒë·ªÉ UI bi·∫øt l√† l·ªói v√† hi·ªÉn th·ªã tr·∫°ng th√°i m·∫∑c ƒë·ªãnh/l·ªói
    return null;
  }
};

export const chatWithAI = async (
  messages: Message[],
  profile: StudentProfile,
  attachments?: { data: string; mimeType: string }[]
) => {
  try {
    const ai = getAIClient();
    const history = messages.slice(0, -1).map((m) => ({
      role: m.role,
      parts: [{ text: m.text }],
    }));

    const lastMessage = messages[messages.length - 1];
    const currentParts: any[] = [{ text: lastMessage.text }];
    if (attachments && attachments.length > 0) {
      attachments.forEach(att => {
        currentParts.push({
          inlineData: {
            data: att.data,
            mimeType: att.mimeType
          }
        });
      });
    }

    const response = await ai.models.generateContent({
      model: "gemini-3-flash-preview",
      contents: [
        ...history,
        { role: 'user', parts: currentParts }
      ],
      config: {
        systemInstruction: `B·∫°n l√† AI Study Copilot cho h·ªçc sinh Vi·ªát Nam. 
H·ªó tr·ª£ gi·∫£i b√†i qua h√¨nh ·∫£nh, ph√¢n t√≠ch t√†i li·ªáu. 
C√¥ng th·ª©c to√°n PH·∫¢I d√πng LaTeX $...$.
Th√¥ng tin: ${JSON.stringify(profile)}.`,
      },
    });
    return response.text;
  } catch (error: any) {
    return "H·ªá th·ªëng ƒëang b·∫≠n, th·ª≠ l·∫°i sau nh√©! üöÄ";
  }
};

export const chatWithAIStream = async (
  messages: Message[],
  profile: StudentProfile,
  onChunk: (text: string) => void,
  attachments?: { data: string; mimeType: string }[]
) => {
  try {
    const ai = getAIClient();
    const history = messages.slice(0, -1).map((m) => ({
      role: m.role === 'user' ? 'user' : 'model',
      parts: [{ text: m.text }],
    }));

    const lastMessage = messages[messages.length - 1];
    const currentParts: any[] = [{ text: lastMessage.text }];
    if (attachments && attachments.length > 0) {
      attachments.forEach(att => {
        currentParts.push({
          inlineData: {
            data: att.data,
            mimeType: att.mimeType
          }
        });
      });
    }

    const response = await ai.models.generateContentStream({
      model: "gemini-3-flash-preview",
      contents: [
        ...history,
        { role: 'user', parts: currentParts }
      ],
      config: {
        systemInstruction: `B·∫°n l√† AI Study Copilot cho h·ªçc sinh Vi·ªát Nam. 
H·ªó tr·ª£ gi·∫£i b√†i qua h√¨nh ·∫£nh, ph√¢n t√≠ch t√†i li·ªáu. 
C√¥ng th·ª©c to√°n PH·∫¢I d√πng LaTeX $...$.
Th√¥ng tin: ${JSON.stringify(profile)}.`,
      },
    });

    let fullText = "";
    for await (const chunk of response) {
      const chunkText = chunk.text;
      if (chunkText) {
        fullText += chunkText;
        onChunk(fullText);
      }
    }
  } catch (error: any) {
    console.error("Streaming error:", error);
    onChunk("H·ªá th·ªëng ƒëang b·∫≠n, th·ª≠ l·∫°i sau nh√©! üöÄ");
  }
};

export const analyzeDocument = async (base64Data: string, mimeType: string) => {
  try {
    const ai = getAIClient();
    const response = await ai.models.generateContent({
      model: "gemini-3-flash-preview",
      contents: {
        parts: [
          { inlineData: { data: base64Data, mimeType } },
          { text: "T√≥m t·∫Øt t√†i li·ªáu n√†y. C√¥ng th·ª©c d√πng LaTeX $...$." },
        ],
      },
      config: { thinkingConfig: { thinkingBudget: 0 } }
    });
    return response.text;
  } catch (error) {
    return "Kh√¥ng th·ªÉ ph√¢n t√≠ch t√†i li·ªáu.";
  }
};
